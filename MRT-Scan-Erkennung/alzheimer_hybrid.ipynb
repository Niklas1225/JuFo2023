{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install ray --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Import\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import os\n",
    "os.environ[\"TUNE_MAX_PENDING_TRIALS_PG\"] = \"1\"\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import ray\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray import air, tune\n",
    "#ray.shutdown()\n",
    "#ray.init(object_store_memory=20000000000, num_cpus=16, num_gpus=4)\n",
    "#os.environ[\"RAY_ALLOW_SLOW_STORAGE\"] = \"1\"\n",
    "from torch.nn import Parameter\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install pytorch-gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install \"opencv-python-headless<4.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.sobel_cam import sobel_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd839c88df0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seed everything\n",
    "seed = 100\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorMapping(x, name_map=\"magma\"):\n",
    "    cm = plt.get_cmap(name_map)\n",
    "\n",
    "    x_map1 = cm(x[ 0, :, :,].detach().numpy())\n",
    "    \n",
    "    x_map2 = torch.Tensor(x_map1[ :, :, :3]).to(torch.float).transpose(-2, -1).transpose(0, 1)\n",
    "\n",
    "    return x_map2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create transformer\n",
    "#function to add gaussian noise, because pytorch does not have one, just for gaussian blur\n",
    "'''\n",
    "def gauss_noise_tensor(img):\n",
    "    assert isinstance(img, torch.Tensor)\n",
    "    dtype = img.dtype\n",
    "    if not img.is_floating_point():\n",
    "        img = img.to(torch.float32)\n",
    "    \n",
    "    mask = []\n",
    "    for i in range(3):\n",
    "        img_part = img[i].to(torch.double)\n",
    "        mask_part = torch.where(img_part > float(img_part[0, 0]), 1., 0.).to(torch.float)\n",
    "        mask.append(mask_part)\n",
    "    \n",
    "    mask = torch.Tensor(np.array([mask_part.detach().numpy() for mask_part in mask]))\n",
    "    \n",
    "    sigma = (0.1**0.8)\n",
    "    \n",
    "    out = img + sigma * mask* torch.randn_like(img)\n",
    "    \n",
    "    if out.dtype != dtype:\n",
    "        out = out.to(dtype)\n",
    "        \n",
    "    return out\n",
    "'''\n",
    "def gauss_noise_tensor(img):\n",
    "    assert isinstance(img, torch.Tensor)\n",
    "    dtype = img.dtype\n",
    "    if not img.is_floating_point():\n",
    "        img = img.to(torch.float32)\n",
    "    \n",
    "    sigma = (0.1**1)\n",
    "    \n",
    "    out = img + sigma * torch.randn_like(img)\n",
    "    \n",
    "    if out.dtype != dtype:\n",
    "        out = out.to(dtype)\n",
    "        \n",
    "    return out\n",
    "\n",
    "transform_normal = transforms.Compose([transforms.Resize(224),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                colorMapping\n",
    "                               ])\n",
    "\n",
    "transform_gauss = transforms.Compose([\n",
    "                                gauss_noise_tensor\n",
    "                               ])\n",
    "\n",
    "#Load train data\n",
    "#Get Images loaded\n",
    "dataset_nonDemented = datasets.ImageFolder(\"../Alzheimer/Dataset2/NonDemented/\", transform=transform_normal)\n",
    "#get count of instances in dir\n",
    "num_nonDemented = len(os.listdir(path=\"./Dataset2/NonDemented/NonDemented\"))\n",
    "#create label data\n",
    "labels_nonDemented = [np.array([1,0,0,0])] * num_nonDemented\n",
    "#exchange targets from dataset with selfmade targets\n",
    "dataset_nonDemented.targets = labels_nonDemented\n",
    "\n",
    "dataset_veryMildDemented = datasets.ImageFolder(\"../Alzheimer/Dataset2/VeryMildDemented/\", transform=transform_normal)\n",
    "num_veryMildDemented = len(os.listdir(path=\"./Dataset2/VeryMildDemented/VeryMildDemented\"))\n",
    "labels_veryMildDemented = [np.array([0,1,0,0])] * num_veryMildDemented\n",
    "dataset_veryMildDemented.targets = labels_veryMildDemented\n",
    "\n",
    "dataset_mildDemented = datasets.ImageFolder(\"../Alzheimer/Dataset2/MildDemented/\", transform=transform_normal)\n",
    "num_mildDemented = len(os.listdir(path=\"./Dataset2/MildDemented/MildDemented\"))\n",
    "labels_mildDemented = [np.array([0,0,1,0])] * num_mildDemented\n",
    "dataset_mildDemented.targets = labels_mildDemented\n",
    "\n",
    "dataset_moderateDemented = datasets.ImageFolder(\"../Alzheimer/Dataset2/ModerateDemented/\", transform=transform_normal)\n",
    "num_moderateDemented = len(os.listdir(path=\"./Dataset2/ModerateDemented/ModerateDemented\"))\n",
    "labels_moderateDemented = [np.array([0,0,0,1])] * num_moderateDemented\n",
    "dataset_moderateDemented.targets = labels_moderateDemented\n",
    "\n",
    "'''\n",
    "print(\"Length of nonDemented-Data_train: \", int(num_nonDemented*(1-test_slice)))\n",
    "print(\"Length of veryMildDemented-Data_train: \", int(num_veryMildDemented*(1-test_slice)))\n",
    "print(\"Length of mildDemented-Data_train: \", int(num_mildDemented*(1-test_slice)))\n",
    "print(\"Length of moderateDemented-Data_train: \", int(num_moderateDemented*(1-test_slice)))\n",
    "print()\n",
    "print(\"Length of nonDemented-Data_test: \", int(num_nonDemented*test_slice))\n",
    "print(\"Length of veryMildDemented-Data_test: \", int(num_veryMildDemented*test_slice))\n",
    "print(\"Length of mildDemented-Data_test: \", int(num_mildDemented*test_slice))\n",
    "print(\"Length of moderateDemented-Data_test: \", int(num_moderateDemented*test_slice))\n",
    "\n",
    "\n",
    "train_data = []\n",
    "for i in range(int(num_nonDemented*test_slice), num_nonDemented):\n",
    "    train_data.append([transform_gauss(dataset_nonDemented[i][0]), labels_nonDemented[i]])\n",
    "for i in range(int(num_veryMildDemented*test_slice), num_veryMildDemented):\n",
    "    train_data.append([transform_gauss(dataset_veryMildDemented[i][0]), labels_veryMildDemented[i]])\n",
    "for i in range(int(num_mildDemented*test_slice), num_mildDemented):\n",
    "    train_data.append([transform_gauss(dataset_mildDemented[i][0]), labels_mildDemented[i]])\n",
    "for i in range(int(num_moderateDemented*test_slice), num_moderateDemented):\n",
    "    train_data.append([transform_gauss(dataset_moderateDemented[i][0]), labels_moderateDemented[i]])\n",
    "'''\n",
    "    \n",
    "train_data = []\n",
    "for i in range(num_nonDemented):\n",
    "    train_data.append([dataset_nonDemented[i][0], labels_nonDemented[i]])\n",
    "for i in range(num_veryMildDemented):\n",
    "    train_data.append([dataset_veryMildDemented[i][0], labels_veryMildDemented[i]])\n",
    "for i in range(num_mildDemented):\n",
    "    train_data.append([dataset_mildDemented[i][0], labels_mildDemented[i]])\n",
    "for i in range(num_moderateDemented):\n",
    "    train_data.append([dataset_moderateDemented[i][0], labels_moderateDemented[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training-data:  4896\n",
      "Length of validation-data:  864\n",
      "Length of test-data:  640\n"
     ]
    }
   ],
   "source": [
    "#shuffle train_data\n",
    "random.shuffle(train_data)\n",
    "\n",
    "#Split train-data into val and train-data\n",
    "test_slice = 0.1 #20%\n",
    "test_data = train_data[0:int(len(train_data) * test_slice)]\n",
    "train_data = train_data[int(len(train_data) * test_slice):]\n",
    "\n",
    "#Split train-data into val and train-data\n",
    "val_slice = 0.15 #20%\n",
    "val_data = train_data[0:int(len(train_data) * val_slice)]\n",
    "train_data = train_data[int(len(train_data) * val_slice):]\n",
    "\n",
    "print(\"Length of training-data: \", len(train_data))\n",
    "print(\"Length of validation-data: \",len(val_data))\n",
    "print(\"Length of test-data: \", len(test_data))\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "#Create dataset and dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last = True, num_workers=8)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True, drop_last = True, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, drop_last = True, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #init convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 16, 2, 2)#3 input_layers, 16 output_layers, 3 kernel size, 2 step size\n",
    "        self.conv2 = nn.Conv2d(16, 32, 2, 2)#16 input_layers, 32 output_layers, 3 kernel size, 2 step size\n",
    "        self.conv3 = nn.Conv2d(32, 64, 2, 2)#32 input_layers, 64 output_layers, 3 kernel size, 2 step size\n",
    "        self.conv4 = nn.Conv2d(64, 128, 2, 2)#64 input_layers, 256 output_layers, 3 kernel size, 2 step size\n",
    "\n",
    "        #init batch normalization\n",
    "        self.norm1 = nn.BatchNorm2d(16, affine=True)#affine= True means with learnable parameters\n",
    "        self.norm2 = nn.BatchNorm2d(32, affine=True)#affine= True means with learnable parameters\n",
    "        self.norm3 = nn.BatchNorm2d(64, affine=True)#affine= True means with learnable parameters\n",
    "        self.norm4 = nn.BatchNorm2d(128, affine=True)#affine= True means with learnable parameters\n",
    "        \n",
    "        self.downsample1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=1, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "        \n",
    "        self.downsample2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=1, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(32)\n",
    "        )\n",
    "        \n",
    "        self.downsample3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=1, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        \n",
    "        self.downsample4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=1, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Layer 1\n",
    "        residual = x\n",
    "        residual  = self.downsample1(residual)\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = residual + x\n",
    "        #print(x.size())\n",
    "        \n",
    "        #Layer2\n",
    "        residual = x\n",
    "        residual  = self.downsample2(residual)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = residual + x\n",
    "        #print(x.size())\n",
    "\n",
    "        #Layer3 \n",
    "        residual = x\n",
    "        residual  = self.downsample3(residual)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = F.relu(x)\n",
    "        x = residual + x\n",
    "        #print(x.size())\n",
    "\n",
    "        #Layer 4\n",
    "        residual = x\n",
    "        residual  = self.downsample4(residual)\n",
    "        x = self.conv4(x)\n",
    "        x = self.norm4(x)\n",
    "        x = F.relu(x)\n",
    "        x = residual + x\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "\n",
    "        #Reshape\n",
    "        x = torch.reshape(x, (x.size(0), x.size(1), x.size(2) * x.size(3)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention(queries, keys, values):\n",
    "    scale = queries.shape[1] ** -0.5\n",
    "\n",
    "    attention_scores = (queries @ keys.transpose(-2, -1)) * scale\n",
    "\n",
    "    attention_probabilities = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "    attention = attention_probabilities @ values\n",
    "\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueriesKeysValuesExtractor(nn.Module):\n",
    "    def __init__(self, token_dim, head_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        queries_keys_values_dim = 3* self.head_dim * self.n_heads\n",
    "        self.input_to_queries_keys_values = nn.Linear(\n",
    "            in_features=token_dim, \n",
    "            out_features=queries_keys_values_dim, \n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, n_tokens, token_dim = x.shape\n",
    "\n",
    "        queries_keys_values = self.input_to_queries_keys_values(x)\n",
    "\n",
    "        queries_keys_values = queries_keys_values.reshape(batch_size, 3, self.n_heads, n_tokens, self.head_dim)\n",
    "\n",
    "        queries, keys, values = queries_keys_values.unbind(dim=1)\n",
    "\n",
    "        return queries, keys, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, token_dim, head_dim, n_heads, dropout_p):\n",
    "        super().__init__()\n",
    "        self.qkv_extractor = QueriesKeysValuesExtractor(\n",
    "            token_dim=token_dim,\n",
    "            head_dim=head_dim,\n",
    "            n_heads = n_heads\n",
    "        )\n",
    "\n",
    "        self.concatenated_heads_dim = n_heads * head_dim\n",
    "\n",
    "        self.attention_to_output = nn.Linear(in_features=self.concatenated_heads_dim, out_features=token_dim)\n",
    "\n",
    "        self.output_dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        batch_size, n_tokens, token_dim = x.shape\n",
    "\n",
    "        queries, keys, values = self.qkv_extractor(x)\n",
    "\n",
    "        attention = get_attention(queries=queries, keys=keys, values=values)\n",
    "        attention_map = attention.clone()\n",
    "        attention = attention.reshape(batch_size, n_tokens, self.concatenated_heads_dim)\n",
    "\n",
    "        x = self.attention_to_output(attention)\n",
    "        x = self.output_dropout(x)\n",
    "        \n",
    "        if return_attention == True:\n",
    "            return x, attention_map\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, token_dim, mhsa_head_dim, mhsa_n_dim, multilayer_perceptron_dim, dropout_p, out_features):\n",
    "        super().__init__()\n",
    "        #init layer\n",
    "        self.out_features=out_features\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(normalized_shape=token_dim)\n",
    "\n",
    "        self.multi_head_self_attention = MultiHeadSelfAttention(\n",
    "            token_dim=token_dim,\n",
    "            head_dim = mhsa_head_dim,\n",
    "            n_heads = mhsa_n_dim,\n",
    "            dropout_p = dropout_p\n",
    "        )\n",
    "\n",
    "        self.layer_norm2 = nn.LayerNorm(normalized_shape=token_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=token_dim, out_features=multilayer_perceptron_dim),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_features=multilayer_perceptron_dim, out_features=out_features)\n",
    "            #Maybe add extra dropout?\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.layer_norm1(x)\n",
    "        x = self.multi_head_self_attention(x)\n",
    "        x = x + residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        if self.out_features != 1:\n",
    "            x = x + residual\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, token_dim, mhsa_head_dim, mhsa_n_dim, multilayer_perceptron_dim, dropout_p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer_block1 = TransformerBlock(token_dim, mhsa_head_dim, mhsa_n_dim, multilayer_perceptron_dim, dropout_p, out_features=token_dim)\n",
    "\n",
    "        self.transformer_block2 = TransformerBlock(token_dim, mhsa_head_dim, mhsa_n_dim, multilayer_perceptron_dim, dropout_p, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer_block1(x)\n",
    "\n",
    "        x = self.transformer_block2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def get_attention_maps(self, x):\n",
    "        attention_maps = []\n",
    "        \n",
    "        _, attn_map1 = self.transformer_block1.multi_head_self_attention(x, return_attention=True)\n",
    "        attention_maps.append(attn_map1)\n",
    "        x = self.transformer_block1(x)\n",
    "        \n",
    "        _, attn_map2 = self.transformer_block2.multi_head_self_attention(x, return_attention=True)\n",
    "        attention_maps.append(attn_map2)\n",
    "        x = self.transformer_block2(x)\n",
    "        \n",
    "        return attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layerNorm = nn.LayerNorm(normalized_shape=in_features)\n",
    "\n",
    "        self.linear = nn.Linear(in_features=in_features, out_features=num_classes)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.layerNorm(x)\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenConcatenator(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        class_token = torch.zeros((batch_size, 128, 1))\n",
    "        self.class_token = Parameter(class_token)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        class_token = self.class_token\n",
    "        \n",
    "        x = torch.cat((x, class_token), dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        position_embedding = torch.zeros((1, 128, 197))\n",
    "        self.position_embedding = Parameter(position_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.position_embedding\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_T(nn.Module):\n",
    "    def __init__(self, lr, batch_size, head_dim, mhsa_n_dim, multilayer_perceptron_dim, dropout_p):\n",
    "        super(CNN_T, self).__init__()\n",
    "        #init variables\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.head_dim = head_dim\n",
    "        self.mhsa_n_dim = mhsa_n_dim\n",
    "        self.multilayer_perceptron_dim = multilayer_perceptron_dim\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        #create the model\n",
    "        #self._create_model()\n",
    "        \n",
    "    def create_model(self):\n",
    "        self.convNet = ConvNet()\n",
    "\n",
    "        self.patchEmbedding = PatchEmbedding()\n",
    "\n",
    "        self.tokenConcatenator = TokenConcatenator(batch_size=self.batch_size)\n",
    "\n",
    "        self.positionEmbedding = PositionEmbedding()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            token_dim=197,#must stay the same\n",
    "            mhsa_head_dim=self.head_dim,#can be changed\n",
    "            mhsa_n_dim=self.mhsa_n_dim,#can be changed\n",
    "            multilayer_perceptron_dim=self.multilayer_perceptron_dim,#can be changed\n",
    "            dropout_p=self.dropout_p#can be changed\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((64, 1))\n",
    "\n",
    "        self.mlp_classifier = MLPClassifier(64, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        #convolutional model\n",
    "        x = self.convNet(x)\n",
    "        #print(x.size())\n",
    "\n",
    "        #patch embedding\n",
    "        x = self.patchEmbedding(x)\n",
    "        #print(x.size())\n",
    "\n",
    "        #concat with token\n",
    "        x = self.tokenConcatenator(x)\n",
    "        #print(x.size())\n",
    "\n",
    "        #positional embedding\n",
    "        x = self.positionEmbedding(x)\n",
    "        #print(x.size())\n",
    "\n",
    "        #transformer block\n",
    "        x = self.encoder(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        \n",
    "        #Reshape out_encoder\n",
    "        x = torch.reshape(x, (x.size(0), x.size(1)))\n",
    "        #print(x.size())\n",
    "        \n",
    "        \n",
    "        \n",
    "        #mlp\n",
    "        x = self.mlp_classifier(x)\n",
    "        #print(x.size())\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_iteration(model, criterion, optimizer, train_loader):\n",
    "    train_losses = 0\n",
    "    train_accuracy = 0\n",
    "    \n",
    "    for inp_data, labels in train_loader:\n",
    "        #reset optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #get data into right shape\n",
    "        labels = torch.tensor(labels, dtype=torch.float, device =device)\n",
    "        inp_data = inp_data.to(device).to(torch.float)\n",
    "        \n",
    "        #get predictions from the model\n",
    "        preds = model(inp_data)\n",
    "        \n",
    "        #calculate the loss and accuracy\n",
    "        loss = criterion(labels, preds)\n",
    "        accuracy = (preds.argmax(dim=-1) == labels.argmax(dim=-1)).float().mean()\n",
    "        \n",
    "        #for the average loss\n",
    "        train_losses += loss.item()\n",
    "        train_accuracy += accuracy\n",
    "        \n",
    "        #calculate the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        #perform the backpropagation\n",
    "        optimizer.step()\n",
    "    \n",
    "    #calculate the average loss\n",
    "    avg_loss = train_losses/len(train_loader)\n",
    "    avg_acc = train_accuracy/len(train_loader)\n",
    "    \n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_one_iteration(model, criterion, val_loader):\n",
    "    val_losses = 0\n",
    "    val_accuracy = 0\n",
    "    \n",
    "    for inp_data, labels in val_loader:\n",
    "        #get data into right shape\n",
    "        labels = torch.tensor(labels, dtype=torch.float, device =device)\n",
    "        inp_data = inp_data.to(device).to(torch.float)\n",
    "        \n",
    "        #get predictions from the model\n",
    "        preds = model(inp_data)\n",
    "        \n",
    "        #calculate the loss and accuracy\n",
    "        loss = criterion(labels, preds)\n",
    "        accuracy = (preds.argmax(dim=-1) == labels.argmax(dim=-1)).float().mean()\n",
    "        \n",
    "        #for the average loss\n",
    "        val_losses += loss.item()\n",
    "        val_accuracy += accuracy\n",
    "        \n",
    "    #calculate the average loss\n",
    "    avg_loss = val_losses/len(val_loader)\n",
    "    avg_acc = val_accuracy/len(val_loader)\n",
    "    \n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_specific_model(config):\n",
    "    #get variables\n",
    "    lr = config[\"lr\"]\n",
    "    head_dim = config[\"head_dim\"]\n",
    "    mhsa_n_dim = config[\"mhsa_n_dim\"]\n",
    "    multilayer_perceptron_dim = config[\"multilayer_perceptron_dim\"]\n",
    "    dropout_p = config[\"dropout_p\"]\n",
    "    \n",
    "    data_id = config[\"data_id\"]\n",
    "    model_id = config[\"model_id\"]\n",
    "    \n",
    "    #epochs = config[\"epochs\"]\n",
    "    #get data\n",
    "    train_data, val_data = ray.get(data_id)\n",
    "    \n",
    "    batch_size = 100\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "    #init the model\n",
    "    #model = nn.DataParallel(CNN_T(lr, 100, head_dim)).to(device)\n",
    "    #get and init model\n",
    "    model = ray.get(model_id)\n",
    "    model.lr = lr\n",
    "    model.head_dim = head_dim\n",
    "    model.mhsa_n_dim = mhsa_n_dim\n",
    "    model.multilayer_perceptron_dim = multilayer_perceptron_dim\n",
    "    model.dropout_p = dropout_p\n",
    "    model.create_model()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    #init criterion and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    #start the training\n",
    "    for i in range(100):\n",
    "        #training\n",
    "        train_losses = 0\n",
    "        train_accuracy = 0\n",
    "\n",
    "        for inp_data, labels in train_loader:\n",
    "            #reset optimizer\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #get data into right shape\n",
    "            labels = labels.to(device).to(torch.float)\n",
    "            inp_data = inp_data.to(device).to(torch.float)\n",
    "\n",
    "            #get predictions from the model\n",
    "            preds = model(inp_data)\n",
    "\n",
    "            #calculate the loss and accuracy\n",
    "            loss = criterion(labels, preds)\n",
    "            accuracy = (preds.argmax(dim=-1) == labels.argmax(dim=-1)).cpu().float().mean()\n",
    "\n",
    "            #for the average loss\n",
    "            train_losses += loss.item()\n",
    "            train_accuracy += accuracy\n",
    "\n",
    "            #calculate the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #perform the backpropagation\n",
    "            optimizer.step()\n",
    "        #calculate the average loss\n",
    "        avg_train_loss = train_losses/len(train_loader)\n",
    "        avg_train_acc = train_accuracy/len(train_loader)\n",
    "        \n",
    "        val_losses = 0\n",
    "        val_accuracy = 0\n",
    "\n",
    "        for inp_data, labels in val_loader:\n",
    "            #get data into right shape\n",
    "            labels = labels.to(device).to(torch.float)\n",
    "            inp_data = inp_data.to(device).to(torch.float)\n",
    "\n",
    "            #get predictions from the model\n",
    "            preds = model(inp_data)\n",
    "\n",
    "            #calculate the loss and accuracy\n",
    "            loss = criterion(labels, preds)\n",
    "            accuracy = (preds.argmax(dim=-1) == labels.argmax(dim=-1)).cpu().float().mean()\n",
    "\n",
    "            #for the average loss\n",
    "            val_losses += loss.item()\n",
    "            val_accuracy += accuracy.item()\n",
    "\n",
    "        #calculate the average loss\n",
    "        avg_val_loss = val_losses/len(val_loader)\n",
    "\n",
    "        avg_val_acc = val_accuracy/len(val_loader)\n",
    "        \n",
    "        #validation\n",
    "        #avg_val_loss, avg_val_acc = val_one_iteration(model, criterion, val_loader)\n",
    "        \n",
    "        #tune.report(loss=avg_val_loss, accuracy=avg_val_acc)\n",
    "        print(\"loss=\", avg_val_loss, \"accuracy=\", avg_val_acc)\n",
    "    \n",
    "    print(\"Finished Training Model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 16:13:16,749\tWARNING services.py:1732 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 1062014976 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2023-01-07 16:13:17,484\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:146: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:172.)\n",
      "  return default_collate([torch.as_tensor(b) for b in batch])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss= 0.13186388742178679 accuracy= 0.5575000010430813\n",
      "loss= 0.11208627372980118 accuracy= 0.6850000023841858\n",
      "loss= 0.09940149262547493 accuracy= 0.7262499928474426\n",
      "loss= 0.0739820240996778 accuracy= 0.7899999991059303\n",
      "loss= 0.04813862591981888 accuracy= 0.8799999952316284\n",
      "loss= 0.03351906826719642 accuracy= 0.90625\n",
      "loss= 0.02762620011344552 accuracy= 0.9262499958276749\n",
      "loss= 0.023007740965113044 accuracy= 0.9474999904632568\n",
      "loss= 0.013586467946879566 accuracy= 0.9675000160932541\n",
      "loss= 0.014693968463689089 accuracy= 0.9612500071525574\n",
      "loss= 0.01140141935320571 accuracy= 0.96875\n",
      "loss= 0.009886135609121993 accuracy= 0.9700000062584877\n",
      "loss= 0.008563363167922944 accuracy= 0.9750000089406967\n",
      "loss= 0.007379356960882433 accuracy= 0.9787500128149986\n",
      "loss= 0.007790114701492712 accuracy= 0.9775000140070915\n",
      "loss= 0.008451372181298211 accuracy= 0.9737500101327896\n",
      "loss= 0.007867849781177938 accuracy= 0.9750000163912773\n",
      "loss= 0.006615920865442604 accuracy= 0.9787500202655792\n",
      "loss= 0.007723204878857359 accuracy= 0.9737500101327896\n",
      "loss= 0.0073493231611792 accuracy= 0.9775000065565109\n",
      "loss= 0.007379037066129968 accuracy= 0.9775000065565109\n",
      "loss= 0.007386146720818942 accuracy= 0.973750002682209\n",
      "loss= 0.007732788391876966 accuracy= 0.9775000065565109\n",
      "loss= 0.008212406391976401 accuracy= 0.9712500050663948\n",
      "loss= 0.007315046736039221 accuracy= 0.9762500077486038\n",
      "loss= 0.0077137759944889694 accuracy= 0.9762500151991844\n",
      "loss= 0.007371284766122699 accuracy= 0.9750000089406967\n",
      "loss= 0.0071592750027775764 accuracy= 0.9750000089406967\n",
      "loss= 0.00799781212117523 accuracy= 0.9737500175833702\n",
      "loss= 0.006996885094849858 accuracy= 0.9750000089406967\n",
      "loss= 0.007724800321739167 accuracy= 0.9762500151991844\n",
      "loss= 0.0070090797962620854 accuracy= 0.9737500175833702\n",
      "loss= 0.007286845851922408 accuracy= 0.9750000089406967\n",
      "loss= 0.007630863867234439 accuracy= 0.9750000238418579\n",
      "loss= 0.00734006951097399 accuracy= 0.9762500151991844\n",
      "loss= 0.007052977009152528 accuracy= 0.9762500151991844\n",
      "loss= 0.007628558101714589 accuracy= 0.9750000089406967\n",
      "loss= 0.007845653461117763 accuracy= 0.9737500101327896\n",
      "loss= 0.007638807757757604 accuracy= 0.9750000089406967\n",
      "loss= 0.00782775180414319 accuracy= 0.9750000089406967\n",
      "loss= 0.007260779922944494 accuracy= 0.9750000163912773\n",
      "loss= 0.007006177267612657 accuracy= 0.9762500077486038\n",
      "loss= 0.00749166333116591 accuracy= 0.9725000113248825\n",
      "loss= 0.006910184267326258 accuracy= 0.9750000163912773\n",
      "loss= 0.007735488063190132 accuracy= 0.9725000113248825\n",
      "loss= 0.007930911611765623 accuracy= 0.9737500175833702\n",
      "loss= 0.007341419957811013 accuracy= 0.9762500077486038\n",
      "loss= 0.007488286442821845 accuracy= 0.9750000089406967\n",
      "loss= 0.006583895796211436 accuracy= 0.976250022649765\n",
      "loss= 0.007581777463201433 accuracy= 0.973750002682209\n",
      "loss= 0.0071621627430431545 accuracy= 0.9775000214576721\n",
      "loss= 0.007276627715327777 accuracy= 0.9750000163912773\n",
      "loss= 0.007693845909670927 accuracy= 0.9750000089406967\n",
      "loss= 0.007449708980857395 accuracy= 0.973750002682209\n",
      "loss= 0.00720399254350923 accuracy= 0.9775000140070915\n",
      "loss= 0.007337728020502254 accuracy= 0.9750000163912773\n",
      "loss= 0.007343630466493778 accuracy= 0.9750000089406967\n",
      "loss= 0.006994215917075053 accuracy= 0.9787500128149986\n",
      "loss= 0.007483520241294173 accuracy= 0.973750002682209\n",
      "loss= 0.006923463224666193 accuracy= 0.9775000140070915\n",
      "loss= 0.007207548915175721 accuracy= 0.9737500101327896\n",
      "loss= 0.007323364843614399 accuracy= 0.9750000163912773\n",
      "loss= 0.007508935945224948 accuracy= 0.9750000014901161\n",
      "loss= 0.006814381296862848 accuracy= 0.9762500151991844\n",
      "loss= 0.007081144576659426 accuracy= 0.9775000214576721\n",
      "loss= 0.00723877142809215 accuracy= 0.973750002682209\n",
      "loss= 0.0076830776743008755 accuracy= 0.9737500101327896\n",
      "loss= 0.00728510387125425 accuracy= 0.9737500101327896\n",
      "loss= 0.007368385384324938 accuracy= 0.9750000238418579\n",
      "loss= 0.006890693155582994 accuracy= 0.9775000065565109\n",
      "loss= 0.006816728564444929 accuracy= 0.9775000140070915\n",
      "loss= 0.007450028351740912 accuracy= 0.9737500175833702\n",
      "loss= 0.007571085763629526 accuracy= 0.9750000089406967\n",
      "loss= 0.006403741834219545 accuracy= 0.9775000140070915\n",
      "loss= 0.007571422422188334 accuracy= 0.973750002682209\n",
      "loss= 0.007343634468270466 accuracy= 0.9737500101327896\n",
      "loss= 0.006540624221088365 accuracy= 0.9800000190734863\n",
      "loss= 0.007001192891038954 accuracy= 0.9750000089406967\n",
      "loss= 0.007401635346468538 accuracy= 0.9737500175833702\n",
      "loss= 0.006873232050566003 accuracy= 0.9762500151991844\n",
      "loss= 0.007335123082157224 accuracy= 0.9762500077486038\n",
      "loss= 0.007029616506770253 accuracy= 0.9762500151991844\n",
      "loss= 0.00733349185611587 accuracy= 0.9750000014901161\n",
      "loss= 0.006581529480172321 accuracy= 0.9787500128149986\n",
      "loss= 0.007473059115000069 accuracy= 0.9750000163912773\n",
      "loss= 0.0063422819948755205 accuracy= 0.9775000140070915\n",
      "loss= 0.00715600821422413 accuracy= 0.9762500077486038\n",
      "loss= 0.007005863095400855 accuracy= 0.9750000089406967\n",
      "loss= 0.007807412948750425 accuracy= 0.9750000014901161\n",
      "loss= 0.0067079914442729205 accuracy= 0.9787500202655792\n",
      "loss= 0.00736857793526724 accuracy= 0.9750000089406967\n",
      "loss= 0.007479301712010056 accuracy= 0.9725000187754631\n",
      "loss= 0.007690910715609789 accuracy= 0.9737500101327896\n",
      "loss= 0.0077790836767235305 accuracy= 0.973750002682209\n",
      "loss= 0.007255533768329769 accuracy= 0.9775000140070915\n",
      "loss= 0.007966350996866822 accuracy= 0.973750002682209\n",
      "loss= 0.006496314948890358 accuracy= 0.9762500151991844\n",
      "loss= 0.006619527470320463 accuracy= 0.9787500128149986\n",
      "loss= 0.006879501459479798 accuracy= 0.9762500002980232\n",
      "loss= 0.006185011428897269 accuracy= 0.9775000065565109\n",
      "Finished Training Model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = CNN_T(0.001, 100, 26, 2, 2, 0.0)\n",
    "model_id = ray.put(model)\n",
    "\n",
    "data_id = ray.put((train_data, val_data))\n",
    "\n",
    "#write config\n",
    "config = {\n",
    "    \"lr\": 0.0017377061778563376,\n",
    "    \"head_dim\": 16,\n",
    "    \"mhsa_n_dim\": 21,\n",
    "    \"multilayer_perceptron_dim\": 29,\n",
    "    \"dropout_p\": 0.0024779216175651675,\n",
    "    \"data_id\": data_id,\n",
    "    \"model_id\": model_id\n",
    "}\n",
    "    \n",
    "\n",
    "model = train_specific_model(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-Accuracy:  0.97666667898496\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "\n",
    "test_accuracy = 0\n",
    "\n",
    "for inp_data, labels in test_loader:\n",
    "    #get data into right shape\n",
    "    labels = labels.to(device).to(torch.float)\n",
    "    inp_data = inp_data.to(device).to(torch.float)\n",
    "\n",
    "    #get predictions from the model\n",
    "    preds = model(inp_data)\n",
    "\n",
    "    accuracy = (preds.argmax(dim=-1) == labels.argmax(dim=-1)).cpu().float().mean()\n",
    "\n",
    "    test_accuracy += accuracy.item()\n",
    "\n",
    "avg_test_acc = test_accuracy / len(test_loader)\n",
    "print(\"Test-Accuracy: \", avg_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7ea6e2631e4169ee9ad4621a924408ab0165a56657c9df100a137db6d171f83f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
